---
layout: post
title: å¤šGPUå¹¶è¡Œè®­ç»ƒ
categories: PyTorch
description: å¤šGPUå¹¶è¡Œè®­ç»ƒ
---

ç›®å‰çš„æƒ…å†µï¼šæœ‰1å°æœºå™¨6å¼ 3090ï¼ˆå•æœºå¤šå¡ï¼‰ï¼Œä½†æ˜¯å®éªŒå®¤å…±ç”¨ï¼Œå¯èƒ½ä¸åŒçš„å¡å‰©ä½™å†…å­˜çš„æƒ…å†µä¸ä¸€æ ·ï¼Œæ‰€ä»¥æƒ³è¦èƒ½æŒ‡å®š1å¼ å¡æˆ–å‡ å¼ å¡è¿›è¡Œè®­ç»ƒã€‚

## é€‰æ‹©ï¼šDataParallel or DistributedDataParallelï¼Ÿ

`DataParalllel`è™½ç„¶ç®€å•ï¼Œä½†æ˜¯å®ƒçš„è´Ÿè½½æ˜¯ä¸å¹³å‡çš„ï¼Œå¯ä»¥çœ‹è¿™ç¯‡æ–‡ç« [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)çš„è§£é‡Šï¼Œç®€çŸ­æ¥è¯´ï¼Œ`DataParallel`ä¸æ¶‰åŠåŒæ­¥ï¼Œåªæ˜¯å°†å¤šä¸ªGPUè®¡ç®—å¾—æ¥çš„æ¢¯åº¦æ±‡æ€»åˆ°ä¸€ä¸ªGPUè¿›è¡Œæ›´æ–°ã€‚å¯¹äºå¤§æ¨¡å‹ä½¿ç”¨`DistributedDataParallel`æ•ˆç‡æ›´é«˜ï¼Œ`DistributedDataParallel`è®¾ç½®äº†åŒæ­¥ï¼ŒçœŸæ­£å®ç°äº†å¹¶è¡Œã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸ç®¡æ˜¯å“ªä¸ªå‡½æ•°ï¼Œè‡ªå®šä¹‰çš„modelä¸€å®šè¦åœ¨`forward`ä¸Šé‡è½½ï¼Œå¦‚æœæœ‰å‡ ä¸ª`forward`æˆ–è€…`forward`å‚æ•°ä¸èƒ½å¯¹æ ‡ï¼Œä¼šå‡ºé—®é¢˜ã€‚æ¯”å¦‚æœ‰å‚æ•°åœ¨`__init__`ä¸­ï¼Œä½†åœ¨`forward`ä¸­ä½¿ç”¨ï¼Œé‚£è¿™äº›å‚æ•°å°±æ²¡åŠæ³•å¹¶è¡Œæ”¾å…¥ï¼Œä¼šå¯¼è‡´æ•°æ®å’Œæ¨¡å‹åˆ†ç¦»ï¼Œä¸åœ¨åŒä¸€ä¸ªGPUä¸Šï¼Œä¼šæŠ¥é”™ï¼ã€‚æ‰€æœ‰åœ¨`forward`ä¸­ä½¿ç”¨çš„å‚æ•°å¿…é¡»ç›´æ¥ä¼ å…¥ï¼Œè¿™æ ·æ‰èƒ½ç›´æ¥ç”¨è¿™ä¸¤ä¸ªå‡½æ•°ã€‚æ¥ä¸‹æ¥ä»‹ç»`DistributedDataParallel`çš„ç”¨æ³•ã€‚

## DistributedDataParallelçš„ç”¨æ³•

ğŸ”—ï¼š[A Comprehensive Tutorial to Pytorch DistributedDataParallel](https://medium.com/codex/a-comprehensive-tutorial-to-pytorch-distributeddataparallel-1f4b42bb1b51)

ä»£ç å¯å‚è€ƒğŸ”—ï¼š[tf-torch-template](https://github.com/taki0112/tf-torch-template/tree/main/pytorch_src)

`DistributedDataParallel`å°†æ¨¡å‹å¤åˆ¶åˆ°Kä¸ªGPUä¸Šï¼ŒæŠŠæ•°æ®æ‹†åˆ°Kä¸ªGPUä¸Šï¼ˆæ•°æ®å¹¶è¡Œï¼‰ã€‚å¯ä»¥åšçš„å‰ææ˜¯æ¨¡å‹åœ¨ä¸€ä¸ªGPUä¸Šæ˜¯æ”¾å¾—ä¸‹çš„ã€‚

1. Setup the process group

   - `process group`ï¼šæœ‰Kä¸ªGPUå°±å¯¹åº”Kä¸ªè¿›ç¨‹
   - `rank`: æ¯ä¸ªè¿›ç¨‹å¯¹åº”çš„idï¼Œä»0åˆ°K-1ï¼Œrank=0æ˜¯master node
   - `world size`ï¼šæ€»å…±çš„è¿›ç¨‹æ•°/GPUæ•°

   ```python
   import torch.distributed as dist
   def setup(rank, world_size):
       os.environ['MASTER_ADDR'] = 'localhost' # or 127.0.0.1
       os.environ['MASTER_PORT'] = '12355'
       # nccl is the most recommand backend
       dist.init_process_group("nccl", rank=rank, world_size=world_size)
   ```

2. Split the dataloader

   dataloaderéœ€è¦æ‹†åˆ†æ•°æ®åˆ°Kä¸ªGPUä¸Šå¹¶ä¸”ä¿è¯ä»–ä»¬ä¸ä¼šæœ‰overlapã€‚å¯ä»¥ç”¨`DistributedSampler`å®ç°ã€‚

   ```python
   from torch.utils.data.distributed import DistributedSampler
   
   # pin_memoryé»˜è®¤æ˜¯falseï¼Œè¿™é‡Œä¸å†™ä¹Ÿè¡Œï¼Œæ„Ÿè§‰è¿™ä¸ªå‚æ•°è·Ÿæˆ‘æ²¡ä»€ä¹ˆå…³ç³»
   def prepare(rank, world_size, batch_size=32, pin_memory=False,num_workers=0):
       dataset = Your_Dataset()
       sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)
       dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=pin_memory, num_workers=num_workers, drop_last=False, shuffle=False, sampler=sampler)
       
       return dataloader
   ```

3. Wrap the model with DDP

   ```python
   from torch.nn.parallel import DistributedDataParallel as DDP
   
   def main(rank, world_size):
       # setup the process groups
       setup(rank, world_size)
       # prepare the dataloader
       dataloader = prepare(rank, world_size)
       
       # instantiate the model(it's your own model) and move it to the right device
       model = Model().to(rank)
       
       # wrap the model with DDP
       # device_ids tell DDP where is your model
       # output_device tells DDP where to output, in our case, it is rank
       # find_unused_parameters=True instructs DDP to find unused output of the forward() function of any module in the model
       model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)
   ```

   è®­ç»ƒç»“æŸåï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ä»checkpointä¸­åŠ è½½æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦åŠ ä¸Š`model.module`ã€‚ä¸‹é¢è¿™æ®µä»£ç å¯ä»¥è‡ªåŠ¨çš„æŠŠè¿™ä¸ªprefixå»æ‰ï¼Œä»`model.module.xxx`åˆ°`model.xxx`ï¼š

   ```python
   # in case we load a DDP model checkpoint to a non-DDP model
   
   model_dict = OrderedDict()
   pattern = re.compile('module.')
   for k,v in state_dict.items():
       if re.search("module", k):
           model_dict[re.sub(pattern, '', k)] = v
       else:
           model_dict = state_dict
   model.load_state_dict(model_dict)
   ```

4. Train/test our model

   ```python
   optimizer = Your_Optimizer()
   loss_fn = Your_Loss()
   for epoch in epochs:
       # if we are using DistributedSampler, we have to tell it which epoch this is
       dataloader.sampler.set_epoch(epoch)       
           
       for step, x in enumerate(dataloader):
           optimizer.zero_grad(set_to_none=True)
               
           pred = model(x)
           label = x['label']
               
           loss = loss_fn(pred, label)
           loss.backward()
           optimizer.step()
   cleanup()
   
   import torch.multiprocessing as mp
   if __name__ == '__main__':
       # suppose we have 3 gpus
       world_size = 3    
       mp.spawn(
           main,
           args=(world_size),
           nprocs=world_size
       )
   ```

   å¦‚æœæƒ³è¦æŒ‡å®šGPUè€Œä¸æ˜¯é¡ºåºç”¨GPUï¼Œéœ€è¦åœ¨torch importä¹‹å‰è®¾å®šå¥½ç¯å¢ƒå˜é‡ï¼š

   ```python
   import os
   os.environ['CUDA_VISIBLE_DEVICES'] = '4, 5, 6, 7'
   ```

â€‹		ä¸Šè¿°æ˜¯ä¸€ä¸ªå¾ˆè¯¦ç»†çš„è§£é‡Šã€‚ä½†æ˜¯æŠŠå®ƒç”¨åˆ°é¡¹ç›®é‡Œå¦‚ä½•å»ç»„ç»‡ï¼Ÿå¯ä»¥å‚è€ƒï¼š[tf-torch-template](https://github.com/taki0112/tf-torch-template)ã€‚æˆ‘åœ¨è¿™ä¸ªæ¡†æ¶ä¸Šè¿›è¡Œäº†ä¿®æ”¹ï¼Œæ»¡è¶³æŒ‡å®šGPUçš„åŠŸèƒ½ã€‚å…·ä½“ä¿®æ”¹å¦‚ä¸‹ï¼š

1. åœ¨`config.yaml`ä¸ŠæŒ‡å®š`device_ids`

   ![image-20230316132545818](/images/posts/IMG_7953.png)

2. åœ¨`run.py`çš„å…¥å£å¤„ä¿®æ”¹`world_size`

   ![image-20230316132628756](/images/posts/IMG_7954.png)

3. åœ¨`run_fn`ä¸­æŒ‡å®šGPU

   deviceçš„æ ¼å¼ç±»ä¼¼'cuda:2'ï¼Œå°†deviceä¼ åˆ°modelä¸­å»ã€‚å…¶ä»–å’Œæä¾›çš„æ¨¡æ¿åŸºæœ¬ä¸€è‡´ã€‚

   ![image-20230316132703269](/images/posts/IMG_7955.png)

   è¿™ç§æ–¹æ³•ä¸ç”¨ç¡¬æ€§æŒ‡å®šå¯è§çš„deviceï¼Œæˆ‘è§‰å¾—è¿˜ä¸é”™ï¼Œç›®å‰è¿è¡Œéƒ½æ²¡æœ‰é—®é¢˜ã€‚

   å¦ä¸€ä¸ªéœ€è¦æ³¨æ„çš„é—®é¢˜å°±æ˜¯åœ¨è¿™ç§å¤šGPUå¹¶è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬saveå’Œloadçš„checkpointåœ¨å“ªä¸ªdeviceä¸Šã€‚é»˜è®¤æƒ…å†µä¸‹loadä¼šå°†æ¨¡å‹åŠ è½½åˆ°å½“åˆè®­ç»ƒå­˜å‚¨çš„deviceä¸Šï¼Œå¦‚æœè¿™ä¸ªæ—¶å€™deviceæœ‰å˜åŒ–ï¼Œéœ€è¦æŒ‡å®šæ–°çš„GPU idã€‚



