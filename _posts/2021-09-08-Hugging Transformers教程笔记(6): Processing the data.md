---
layout: post

title: Hugging Face Transformersæ•™ç¨‹ç¬”è®°(6)ï¼šProcessing the data

categories: NLP
description: æ•°æ®é›†
---

[Processing the data](https://huggingface.co/course/chapter3/2?fw=pt)

æ¥ä¸‹æ¥çš„å‡ ç¯‡æ•™ç¨‹ä¼šæ¼”ç¤ºå¦‚ä½•fine tuneä¸€ä¸ªtransformerã€‚è¿™ç¯‡æ•™ç¨‹ä¸»è¦è®²çš„æ˜¯å¦‚ä½•å¤„ç†æ•°æ®é›†ã€‚

## è·å–æ•°æ®é›†

åœ¨è¿™ä¸€ç« æˆ‘ä»¬ä»¥MRPCï¼ˆMicrosoft Research Paraphrase Corpusï¼‰ä¸ºä¾‹ï¼ŒThe dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing).MRPCæ˜¯GLUE benchmarkçš„åä¸ªæ•°æ®é›†ä¹‹ä¸€ï¼ŒGLUEç”¨æ¥è¡¡é‡æ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¡¨ç°ã€‚

Hugging Faceé™¤äº†æœ‰transformer modelä»¥å¤–ï¼Œä¹Ÿæœ‰[æ•°æ®é›†](https://huggingface.co/datasets)å¯ä»¥å¾ˆæ–¹ä¾¿çš„loadè¿›æ¥ï¼š


```python
!pip install datasets transformers[sentencepiece]
```


```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```


    Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s]



    Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]


    Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...



    Downloading: 0.00B [00:00, ?B/s]



    Downloading: 0.00B [00:00, ?B/s]



    Downloading: 0.00B [00:00, ?B/s]



    0 examples [00:00, ? examples/s]



    0 examples [00:00, ? examples/s]



    0 examples [00:00, ? examples/s]


    Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.





    DatasetDict({
        train: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 3668
        })
        validation: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 408
        })
        test: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 1725
        })
    })



ä»è¾“å‡ºå¯ä»¥çœ‹å‡ºï¼ŒMRPCæ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†ï¼ˆ3668 pairsï¼‰ã€éªŒè¯é›†ï¼ˆ408 pairsï¼‰ã€æµ‹è¯•é›†ï¼ˆ1725 pairsï¼‰ã€‚æ ¼å¼ä¸º(sentence1, sentence2, label, idx)ã€‚

å’Œmodelç±»ä¼¼ï¼Œæ•°æ®é›†ä¸‹è½½åç¼“å­˜åœ¨*~/.cache/huggingface/dataset*ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡**HF_HOME**æ¥æ”¹å˜ã€‚

å–æ•°æ®ï¼š


```python
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```




    {'idx': 0,
     'label': 1,
     'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
     'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}



è¿™é‡Œå¯ä»¥çœ‹åˆ°```label```å·²ç»æ˜¯æ•°å­—äº†ï¼Œæˆ‘ä»¬å¦‚æœæƒ³è¦çŸ¥é“ä¸åŒçš„æ•°å­—å¯¹åº”çš„æ˜¯ä»€ä¹ˆæ„ä¹‰å¯ä»¥ï¼š


```python
raw_train_dataset.features
```




    {'idx': Value(dtype='int32', id=None),
     'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
     'sentence1': Value(dtype='string', id=None),
     'sentence2': Value(dtype='string', id=None)}



## æ•°æ®é›†é¢„å¤„ç†


```python
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```


    Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]



    Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]



    Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]



    Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]


æˆ‘ä»¬ä¸èƒ½ç›´æ¥æŠŠtokenized_sentence_1å’Œtokenized_sentence_2ä½œä¸ºè¾“å…¥æ”¾å…¥æ¨¡å‹ï¼Œéœ€è¦ç»„åˆæˆpairæ‰å¯ä»¥ï¼Œè€Œtokenizeræœ¬èº«ä¹Ÿå¯ä»¥å°†å¥å­ç»„åˆæˆpairï¼š


```python
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```




    {'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}



```token_type_ids```æ ‡æ˜ä¸¤å¥è¯çš„ä½ç½®ã€‚


```python
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```




    ['[CLS]',
     'this',
     'is',
     'the',
     'first',
     'sentence',
     '.',
     '[SEP]',
     'this',
     'is',
     'the',
     'second',
     'one',
     '.',
     '[SEP]']



å¯ä»¥çœ‹åˆ°æ ¼å¼ä¸ºï¼š **[CLS] sentence1 [SEP] sentence2 [SEP]**


```python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```




    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]



tokenizeræ˜¯å¦ä¼šè¿”å›```token_type_ids```è¿˜æ˜¯è¦çœ‹é¢„è®­ç»ƒæ¨¡å‹æ˜¯æ€ä¹ˆå¤„ç†çš„ï¼Œæ¯”å¦‚DistillBERTå°±æ˜¯æ²¡æœ‰çš„ã€‚

æˆ‘ä»¬å¯ä»¥è¿™æ ·å¤„ç†è®­ç»ƒé›†ï¼š


```python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

ä¸Šè¿°çš„åšæ³•æœ‰å‡ ä¸ªä¸å¥½çš„åœ°æ–¹ï¼š

1. è¿”å›çš„ç»“æœæ˜¯å­—å…¸ï¼ŒåŒ…å«æœ‰keyï¼šinput_ids, attention_mask, token_type_ids, å€¼ä¸ºlist of listsã€‚
2. åœ¨è¿è¡Œæ—¶éœ€è¦æœ‰è¶³å¤Ÿçš„RAMã€‚
3. è¿™æ ·åšåªèƒ½åˆ†åˆ«å¤„ç†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨```Dataset.map```æ–¹æ³•ã€‚The map method works by applying a function on each element of the dataset.ä¸‹é¢æˆ‘ä»¬å…ˆå®šä¹‰è¿™ä¸ªå‡½æ•°ï¼š


```python
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

å…¶ä¸­```example```æ˜¯ä¸€ä¸ªdictionaryï¼ˆå®ƒçš„keyæ˜¯featuresï¼‰ï¼Œtokenize_functionè¿”å›äº†ä¸€ä¸ªæ–°çš„dictionaryï¼ˆå°†attention_mask, input_ids, token_type_idsåŠ å…¥featuresï¼‰ã€‚å¯ä»¥å›å¿†ä¸€ä¸‹raw_datasetsï¼Œå’Œå¦‚ä¸‹çš„tokenized_datasetsä½œä¸ºå¯¹æ¯”ï¼š


```python
raw_datasets
```




    DatasetDict({
        train: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 3668
        })
        validation: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 408
        })
        test: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 1725
        })
    })




```python
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```


      0%|          | 0/4 [00:00<?, ?ba/s]



      0%|          | 0/1 [00:00<?, ?ba/s]



      0%|          | 0/2 [00:00<?, ?ba/s]





    DatasetDict({
        train: Dataset({
            features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
            num_rows: 3668
        })
        validation: Dataset({
            features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
            num_rows: 408
        })
        test: Dataset({
            features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
            num_rows: 1725
        })
    })



```tokenized_function```ä¸æ­¢å¯ä»¥å¯¹ä¸€ä¸ªelementç”Ÿæ•ˆï¼Œè¿˜å¯ä»¥å¯¹æ•´ä¸ªbatchç”Ÿæ•ˆï¼Œåªéœ€è¦åŠ ä¸Š```batched=True```ï¼Œè¿™æ ·å¯ä»¥åŠ å¿«tokenizerçš„é€Ÿåº¦ã€‚å¦å¤–ï¼Œæˆ‘ä»¬æ²¡æœ‰åœ¨```tokenized_function```é‡Œè®¾ç½®paddingï¼Œæ˜¯å› ä¸ºï¼šå¦‚æœå¯¹æ¯ä¸€ä¸ªsample paddingåˆ°max lengthæ˜¯éå¸¸ä½æ•ˆçš„ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥batchä¸ºå•ä½æ¥paddingï¼Œè¿™æ ·max lengthå¯èƒ½ä¹Ÿä¸ä¼šé‚£ä¹ˆå¤§ï¼Œé€Ÿåº¦ä¹Ÿä¼šå¿«å¾ˆå¤šã€‚

é™¤äº†batchå¯ä»¥æé«˜é€Ÿåº¦å¤–ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šå¤šçº¿ç¨‹æ¥æé«˜æ•ˆç‡ï¼ŒæŒ‡å®š```num_proc```å‚æ•°å°±å¯ä»¥ã€‚ 

We didnâ€™t do this here because the ğŸ¤— Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.

tokenizeré™¤äº†æ·»åŠ äº†ä¸‰é¡¹ç‰¹å¾å¤–ï¼Œä¹Ÿå¯ä»¥åœ¨å‡½æ•°ä¸­æ”¹å˜åŸæœ‰çš„ç‰¹å¾ã€‚

æ¥ä¸‹æ¥è¦åšçš„å°±æ˜¯ä¹‹å‰è®²è¿‡çš„paddingï¼Œå› ä¸ºæŒ‰æ¯ä¸ªbatchæ¥paddingï¼Œæ‰€ä»¥ä¹Ÿå«dynamic paddingã€‚

## Dynamic padding

In PyTorch, the function that is responsible for putting together samples inside a batch is called a **collate function**. 

ä¸Šæ–‡ä¸­æåˆ°çš„æŒ‰ç…§batchæ¥paddingçš„åšæ³•ä¸é€‚åˆTPUï¼ŒTPUéœ€è¦ç›¸åŒå½¢çŠ¶çš„tensorã€‚

collate functionå¯ä»¥åœ¨```DataLoader```å¤„æŒ‰ç…§å‚æ•°ä¼ å…¥å®šä¹‰ï¼Œé»˜è®¤çš„collateåŠæ³•å°±æ˜¯å°†æ ·æœ¬è½¬æ¢ä¸ºtensorï¼Œrecursively if your elements are lists, tuples, or dictionaries.

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨é»˜è®¤çš„collate functionï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æŒ‰ç…§batchæ¥paddingï¼ŒHugging Face Transformersåº“ä¸­æä¾›äº†```DataCollatorWithPadding```å¯ä¾›ä½¿ç”¨ï¼š


```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

è¿™æ˜¯åœ¨paddingä¹‹å‰çš„æƒ…å†µï¼šï¼ˆå–å‰8ä¸ªsamplesï¼ˆä¸€ä¸ªbatchï¼‰ï¼Œçœ‹ä¸€ä¸‹é•¿åº¦ï¼‰


```python
samples = tokenized_datasets["train"][:8]
# stringæ— æ³•è½¬æ¢ä¸ºtensorï¼Œæ‰€ä»¥è¿™é‡Œå»æ‰äº†idx, sentence1, sentence2ä¸‰ä¸ªç‰¹å¾
samples = {
    k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]
}
[len(x) for x in samples["input_ids"]]
```




    [50, 59, 47, 67, 59, 50, 62, 32]



è¿›è¡Œdynamic paddingåï¼š


```python
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```




    {'attention_mask': torch.Size([8, 67]),
     'input_ids': torch.Size([8, 67]),
     'labels': torch.Size([8]),
     'token_type_ids': torch.Size([8, 67])}



å¯ä»¥çœ‹åˆ°éƒ½å˜ä¸ºäº†67ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªbatchä¸­çš„æœ€å¤§é•¿åº¦ã€‚
